{
  
    
        "post0": {
            "title": "Prezzi e rendimenti con Pandas",
            "content": "fastpages, la piattaforma sulla quale è costruito questo blog, permette di integrare direttamente dei notebook di Jupyter nei post. Per provare questa funzionalità, costruiamo un breve notebook su come gestire prezzi e rendimenti delle azioni con Pandas, in alternativa a Excel. In questo modo, potremo osservare in azione alcuni degli strumenti che ho descritto in un mio post precedente. . import pandas as pd import altair as alt import yfinance as yf . Scarichiamo da Yahoo! Finance i prezzi di quattro azioni statunitensi, identificate attraverso i loro ticker: Apple (AAPL), Microsoft (MSFT), McDonald&#39;s (MCD) e Coca-Cola (KO). Due azioni della new economy e due della old, come si diceva qualche anno fa. . tickers = &#39;AAPL MSFT MCD KO&#39; data = yf.download(tickers=tickers, period=&#39;2y&#39;) . [*********************100%***********************] 4 of 4 completed . A questo punto, eliminiamo dal database che abbiamo costruito i prezzi open, high, low e il volume negoziato per tenerci solo i prezzi di chiusura, rettificati dei dividendi eventualmente distribuiti e delle corporate action. Vediamo le prime righe del database dei prezzi. . prices = data.xs(&#39;Adj Close&#39;, axis=1, level=0) prices.tail() . AAPL KO MCD MSFT . Date . 2021-06-17 131.789993 | 54.950001 | 233.880005 | 260.899994 | . 2021-06-18 130.460007 | 53.770000 | 229.619995 | 259.429993 | . 2021-06-21 132.300003 | 54.360001 | 232.899994 | 262.630005 | . 2021-06-22 133.979996 | 54.560001 | 233.880005 | 265.510010 | . 2021-06-23 133.729996 | 54.389999 | 233.725006 | 264.809998 | . Possiamo fare un primo grafico dell&#39;andamento dei prezzi delle quattro azioni negli ultimi due anni, utilizzando Altair, una libreria che permette di produrre grafici interattivi. . alt.Chart( prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento prezzi negli ultimi due anni&#39; ).interactive() . Per avere la possibilità di confrontare i quattro grafici, ribasiamo i dati facendo partire gli andamenti da quota 100. . rebased_prices = prices.div(prices.iloc[0, :]).mul(100) alt.Chart( rebased_prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento prezzi ribasati negli ultimi due anni&#39; ).interactive() . Passiamo ora ai rendimenti e calcoliamo i rendimenti mensili delle quattro azioni. . monthly_returns = prices.resample(&#39;M&#39;).last().pct_change() . Possiamo fare un grafico a barre dei rendimenti appena calcolati, divisi per mese. . alt.Chart( monthly_returns.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Return&#39; ) ).mark_bar().encode( y=&#39;Stock:N&#39;, x=&#39;Return:Q&#39;, color=&#39;Stock:N&#39;, row=&#39;Date:T&#39;, tooltip=[&#39;Stock&#39;, &#39;Return&#39;] ).properties( title=&#39;Rendimenti percentuali per mese&#39; ).interactive() . Oppure lo stesso grafico raggruppato per titolo, dove notiamo come Apple sia stato il titolo più volatile negli ultimi due anni, ma Coca-Cola e McDonald&#39;s abbiano avuto i maggiori drawdown. . alt.Chart( monthly_returns.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Return&#39; ) ).mark_bar().encode( x=&#39;Date:T&#39;, y=&#39;Return:Q&#39;, color=&#39;Stock:N&#39;, row=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Return&#39;] ).properties( title=&#39;Rendimenti percentuali per mese&#39; ).interactive() . Ci fermiamo qui con questo post di prova, che mostra le potenzialità di analisi esplorativa di Pandas, in grado in poche righe di codice di scaricare, elaborare e visualizzare serie storiche di dati con grande facilità. .",
            "url": "https://marcobonifacio.github.io/blog/jupyter/pandas/finanza/2021/06/23/prezzi-e-rendimenti-con-pandas.html",
            "relUrl": "/jupyter/pandas/finanza/2021/06/23/prezzi-e-rendimenti-con-pandas.html",
            "date": " • Jun 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "L&#39;analisi delle componenti principali (principal component analysis o PCA) è una tecnica statistica di riduzione della multidimensionalità dei dati ampiamente utilizzata anche nell&#39;analisi dei mercati finanziari. In questo post andremo ad applicarla a un semplice portafoglio di ETF anche in una versione &quot;dinamica&quot;, avvalendoci della tecnica delle medie mobili esponenziali. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import yfinance as yf # Rendiamo i grafici un po&#39; più carini plt.style.use(&#39;seaborn-darkgrid&#39;) . def normalize(series): return series.subtract(series.mean()).div(series.std()) def denormalize(series, mu, sd): return series.multiply(sd).add(mu) def format_pc(seq): return [f&#39;PC{n:02}&#39; for n, _ in enumerate(seq)] def df_dict(fun, seq): return {s: fun(s) for n, s in enumerate(seq)} def df_list(fun, seq): return [fun(s) for s in seq] def concat_dict(d): return pd.concat(d.values(), axis=0, keys=d.keys()) def dot(df1, df2): return df1.dot(df2.T) def dot3d(df1, df2): return df1.multiply(df2).sum(axis=1).unstack() def cov_matrix(df, norm, ewm, alpha=None): if ewm: if norm: return df.apply(normalize).ewm(alpha=alpha).cov() return df.ewm(alpha=alpha).cov() if norm: return df.apply(normalize).cov() return df.cov() def calc_variance(values): return (values / values.sum()).cumsum() def calc_pca(df): values, vectors = np.linalg.eig(df) return ( pd.Series(calc_variance(values), format_pc(df.index)), pd.DataFrame(vectors, format_pc(df.index), df.columns) ) def calc_ewm_pca(df): ans = df.fillna(0).groupby(level=0).apply(np.linalg.eig) def vectors(date): return pd.DataFrame( np.real(ans.loc[date][1]), format_pc(df.columns), df.columns ) def variance(date): return calc_variance(np.real(ans.loc[date][0])) dfs = df_dict(vectors, ans.index) return ( pd.DataFrame( df_list(variance, ans.index), ans.index, format_pc(df.columns) ), concat_dict(dfs) ) def calc_pc_returns(vectors, returns, norm): if not norm: return dot(returns, vectors) mu = dot(returns.mean(), vectors) sd = dot(returns.std(), vectors) return denormalize(dot(returns.apply(normalize), vectors), mu, sd) def calc_ewm_pc_returns(vectors, returns, norm): if not norm: return returns.T.apply(lambda c: vectors.xs(c.name).dot(c)).T mu = dot3d(vectors, returns.mean()) sd = dot3d(vectors, returns.std()) return denormalize(returns.apply(normalize).T.apply(lambda c: vectors.xs (c.name).dot(c)).T, mu, sd) . Scarichiamo le serie storiche di alcuni ETF di Vanguard, tra cui i quattro che suggerisce come &quot;mattoncini di base&quot; nel portfolio builder dedicato agli investitori americani. Aggiungiamo due ETF dedicati al real estate e alle commodities e scarichiamo anche la serie dei prezzi dell&#39;indice VIX, la volatilità del listino statunitense. . tickers = [ &#39;VTI&#39;, # Vanguard Total Stock Market &#39;VXUS&#39;, # Vanguard Total International Stock &#39;BND&#39;, # Vanguard Total Bond Market &#39;BNDX&#39;, # Vanguard Total International Bond &#39;VNQ&#39;, # Vanguard Real Estate Index Fund &#39;GSG&#39;, # iShares S&amp;P GSCI Commodity-Indexed Trust &#39;^VIX&#39; # Indice VIX ] data = yf.download(tickers=tickers, period=&#39;2y&#39;) prices = data.xs(&#39;Adj Close&#39;, axis=1, level=0) . [*********************100%***********************] 7 of 7 completed . Ribasiamo i prezzi dei nostri ETF a 100 e disegniamo un primo grafico per vederne l&#39;andamento storico. . rebased_prices = prices.iloc[:, :6].div(prices.iloc[0, :6]).mul(100) rebased_prices.plot(figsize=(20, 6), title=&#39;Andamento prezzi ETF ribasati negli ultimi due anni&#39;, subplots=True, layout=(2, 3), sharey=True); . Calcoliamo i rendimenti giornalieri delle serie storiche e la matrice di correlazione tra gli ETF. . returns = rebased_prices.pct_change() returns.corr() . BND BNDX GSG VNQ VTI VXUS . BND 1.000000 | 0.646752 | 0.056605 | 0.217327 | 0.187121 | 0.217059 | . BNDX 0.646752 | 1.000000 | 0.036933 | 0.288205 | 0.207281 | 0.238557 | . GSG 0.056605 | 0.036933 | 1.000000 | 0.380554 | 0.492745 | 0.500353 | . VNQ 0.217327 | 0.288205 | 0.380554 | 1.000000 | 0.857014 | 0.805494 | . VTI 0.187121 | 0.207281 | 0.492745 | 0.857014 | 1.000000 | 0.922180 | . VXUS 0.217059 | 0.238557 | 0.500353 | 0.805494 | 0.922180 | 1.000000 | . Per calcolare le componenti principali, è preferibile utilizzare una matrice di covarianza, a partire dai rendimenti standardizzati), ossia da serie con media nulla e varianza pari a 1. . covmat = cov_matrix(returns, True, False) . Il calcolo matriciale delle componenti principali prevede di trovare autovalori e autovettori della matrice di covarianza, qui calcoliamo già un vettore di autovalori cumulati, che andiamo poi a disegnare come grafico a barre. . variance, vectors = calc_pca(covmat) . Dal grafico possiamo vedere che la prima componente principale (PC00), spiega poco meno del 60% della varianza complessiva dei rendimenti degli indici, mentre la somma delle prime due componenti portano la varianza spiegata quasi all&#39;80%. Chiaramente, in questo portafoglio di 6 strumenti, la necessità di ridurre la dimensionalità è modesta, ma la tecnica è utile laddove ci siano decine o centinaia di indici o titoli per identificare le componenti principali da utilizzare poi per ulteriori analisi. . variance.plot.bar(figsize=(8, 6), title=&#39;Varianza cumulata spiegata dalle componenti principali&#39;); . A partire dalla matrice degli autovettori possiamo inoltre ricostruire i &quot;rendimenti&quot; delle componenti principali e i valori delle serie, come se fossero prezzi di serie finanziarie. Lo facciamo nel grafico successivo, ricordando che le prime due componenti principali sono rappresentate rispettivamente dalla linea blu e da quella arancio. . pc_returns = calc_pc_returns(vectors, returns, True) pc_prices = pc_returns.add(1).cumprod().multiply(100) pc_prices.plot(figsize=(20, 6), title=&#39;Andamento valori componenti principali negli ultimi due anni&#39;, subplots=True, layout=(2, 3), sharey=True); . Le componenti principali sono calcolate sul periodo di analisi e dipendono pertanto dalla finestra temporale scelta. Ci si potrebbe domandare: è possibile averne una versione dinamica, rolling che non dipenda da una finestra temporale fissa? Proviamo ad applicare la tecnica calcolando delle matrici di covarianza rolling con la metodologia delle medie mobili esponenziali, applicando un fattore di decadimento (alpha), pari a 0.06 - per ogni osservazione, il 94% deriva dal dato precedente e il 6% è introdotto sulla base del nuovo dato. Disegniamo poi il grafico della varianza spiegata dalla prima componente principale, mettendolo a confronto con l&#39;andamento dell&#39;indice VIX. . ewm_covmat = cov_matrix(returns, True, True, 0.06) . Interessante: la varianza spiegata dalla prima componente principale arriva fino a più dell&#39;80% (mentre l&#39;ultimo dato è al 50%) esattamente in coincidenza con il picco del VIX, ossia della correzione dei mercati di marzo 2020. Il che è in totale accordo con l&#39;ipotesi che la correlazione dei mercati e delle classi di attivi cresca nei momenti di maggiore volatilità e di correzione. . import warnings warnings.filterwarnings(&#39;ignore&#39;) ewm_variance, ewm_vectors = calc_ewm_pca(ewm_covmat) pd.concat([ewm_variance.iloc[20:, 0], prices.iloc[:, 6]], axis=1).plot(figsize=(8, 6), title=&#39;Confronto andamento prima PC e indice VIX&#39;, subplots=True); . Possiamo infine calcolare gli andamenti storici delle componenti principali, che saranno certamente differenti da quelle &quot;statiche&quot;; in particolare, la prima componente principale è sempre rappresentata in blu nel grafico. . ewm_pc_returns = calc_ewm_pc_returns(ewm_vectors, returns, False) ewm_pc_prices = ewm_pc_returns.add(1).cumprod().multiply(100) ewm_pc_prices.plot(figsize=(20, 6), title=&#39;Andamento valori componenti principali EW negli ultimi due anni&#39;, subplots=True, layout=(2, 3), sharey=True); .",
            "url": "https://marcobonifacio.github.io/blog/2021/05/26/analizzare-i-mercati-con-le-componenti-principali.html",
            "relUrl": "/2021/05/26/analizzare-i-mercati-con-le-componenti-principali.html",
            "date": " • May 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Python per la finanza",
            "content": "Python è per me diventato negli anni uno strumento indispensabile di lavoro, soprattutto quando i dati da gestire superano le capacità di Excel o per effettuare alcune analisi statistiche un po’ più sofisticate del solito. Tuttavia, mi sono accorto che costruire un ambiente di lavoro efficace ed efficiente basato su Python non è sempre intuitivo, per cui proverò in questo post a condividere alcune delle cose che ho imparato nel tempo per giungere a una configurazione soddisfacente di questo importante strumento, soprattutto per chi lavora in ambito finanziario o ha comunque necessità di analizzare dei dati. . La distribuzione di Python più interessante per chi lavora con i dati (o fa data science, come si dice adesso) è sicuramente Anaconda, che però installa di default fin troppe librerie scientifiche per i miei gusti. Esiste una versione minimal di Anaconda, miniconda, che installa solo l’interprete del linguaggio e un set minimo di librerie per il funzionamento del package manager, lasciando all’utente la possibilità di installare poi solo gli ulteriori pacchetti effettivamente necessari per ciascun progetto. Qui però iniziano le difficoltà. Se da un lato conda, rispetto a pip, il package manager ufficiale di Python, permette una gestione delle librerie più agevole, dall’altro il numnero di pacchetti disponibili è ben inferiore. Per fortuna, un’ampia community si è fatta carico di ampliare la disponibilità di librerie sotto conda attraverso il canale conda-forge. Mischiare però le distribuzioni “ufficiali” di Anaconda con quelle di conda-forge è sconsigliabile e nel tempo rischia di corrompere i vari ambienti di lavoro di Python. Per questo ho recentemente scoperto con soddifazione miniforge, un clone di miniconda che scarica direttamente i pacchetti di Python dal canale della community di conda-forge1. Alla fine, quindi, sto usando con soddisfazione da alcuni mesi questa distribuzione, con impostazioni identiche tra i vari dispositivi che utilizzo in modo da avere ambienti di lavoro intercambiabili. . Passando alle librerie vere e proprie, la prima cosa da installare, secondo me, è la triade numpy - pandas - matplotlib. Altamente integrati tra loro, questi tre pacchetti soddisfano tutte le esigenze di analisi e visualizzazione dei dati, sostituendo più che degnamente gli enormi fogli Excel con centinaia di collegamenti che vanno velocemente fuori controllo. A questa configurazione base si possono poi aggiungere numerose altre librerie più specializzate, tra cui, ad esempio, scipy per le procedure di ottimizzazione o scikit-learn per classificazione e regressione (o - più pomposamente - per il machine learning). . Tutto questo può essere gestito tramite un normale editor di testo o l’interprete interattivo di Python, ma è probabilmente più comodo fare le prime analisi di un progetto attraverso un notebook, un’applicazione web che consente di mescolare codice, risultati, visualizzazioni e testi in un unico documento. Qui entra in gioco jupyter, progetto open source di notebook multi-linguaggio, che nell’ultima versione - JupyterLab - integra anche un terminale e vari plugin che lo rendono un vero e proprio ambiente web interattivo di sviluppo. Sebbene i notebook, estremamente versatili proprio per la capacità di integrare codice, testo e grafici in un unico documento, possano essere strumenti decisamente sofisticati in grado di generare un prodotto finito molto elegante, si prestano anche bene per le prime analisi esplorative e per costruire veloci prototipi di progetto, grazie all’immediatezza con cui in poche righe di codice si può caricare una tabella di dati, elaborarla e produrre qualche grafico a corredo. . Il problema dei notebook è che si prestano a uno stile di programmazione imperativo che rischia di produrre documenti “pasticciati” in caso di progetti più complessi e strutturati. Per questi ultimi, sono però disponibili strumenti che permettono di creare con relativa facilità delle vere e proprie applicazioni web in cui risulta più semplice separare parametri e algoritmi e strutturare in modo più funzionale il codice. Tra questi strumenti, citerei in particolare Streamlit che ho recentemente utilizzato per costruire una dashboard nella quale riunire tutti i task quotidiani in modo da avere un vero e proprio pannello di controllo delle attività ricorrenti nella mia giornata lavorativa. Quello che mi ha sorpreso di Streamlit è l’assoluta facilità con cui è stato possibile integrare elementi interattivi nel codice per costruire pagine web dinamiche; inoltre - ma questa è una caratteristica che condivide con molti altri pacchetti - come sia stato possibile costruire delle vere e proprie web app solo con Python2. . Ecco, in sintesi questo è il mio ambiente di lavoro dietro lo schermo. Peraltro, io sono un tipo curioso e ho provato diversi strumenti in questi anni, prima di arrivare a una configurazione più o meno stabile che ho cercato qui di descrivere. E non è detto che non troverò qualcosa di interessante anche domani, per cui cambierò qualcosa e avrò una scusa per tornare a parlarne. . Anche se in realtà lo stesso risultato si può ottenere in :code:miniconda settando il canale :code:conda-forge per primo. &#8617; . | Ovviamente, dietro c’è anche tanto Javascript, ma se ne occupa direttamente :code:Streamlit o chi per lui. &#8617; . |",
            "url": "https://marcobonifacio.github.io/blog/python/finanza/2021/05/05/python-per-la-finanza.html",
            "relUrl": "/python/finanza/2021/05/05/python-per-la-finanza.html",
            "date": " • May 5, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://marcobonifacio.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marcobonifacio.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}