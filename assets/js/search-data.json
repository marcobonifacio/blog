{
  
    
        "post0": {
            "title": "Componenti principali e classificazione",
            "content": "In questo post indagheremo la possibilità di utilizzare tecniche di machine learning e in particolare algoritmi di classificazione per l&#39;analisi dei mercati azionari e il raggruppamento di titoli in base alla loro esposizione alle componenti principali dei mercati, che abbiamo già visto in precedenza. Utilizziamo a tal fine i primi venti titoli che compongono l&#39;indice FTSEMIB dei quali scarichiamo da Yahoo! Finance le serie storiche degli ultimi due anni, trasformandole infine in rendimenti giornalieri. . import pandas as pd import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import altair as alt import yfinance as yf . . tickers = [ &#39;ENEL.MI&#39;, &#39;ISP.MI&#39;, &#39;STLA.MI&#39;, &#39;ENI.MI&#39;, &#39;UCG.MI&#39;, &#39;G.MI&#39;, &#39;RACE.MI&#39;, &#39;STM.MI&#39;, &#39;CNHI.MI&#39;, &#39;MONC.MI&#39;, &#39;SRG.MI&#39;, &#39;FBK.MI&#39;, &#39;TRN.MI&#39;, &#39;NEXI.MI&#39;, &#39;PRY.MI&#39;, &#39;ATL.MI&#39;, &#39;MB.MI&#39;, &#39;EXO.MI&#39;, &#39;CPR.MI&#39;, &#39;AMP.MI&#39; ] . . data = yf.download(tickers=tickers, period=&#39;2y&#39;) prices = data.xs(&#39;Adj Close&#39;, axis=1, level=0) returns = prices.pct_change() returns.head() . . [*********************100%***********************] 20 of 20 completed . AMP.MI ATL.MI CNHI.MI CPR.MI ENEL.MI ENI.MI EXO.MI FBK.MI G.MI ISP.MI MB.MI MONC.MI NEXI.MI PRY.MI RACE.MI SRG.MI STLA.MI STM.MI TRN.MI UCG.MI . Date . 2019-06-28 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2019-07-01 0.001945 | -0.032300 | 0.013972 | 0.012188 | -0.002280 | 0.002328 | 0.007792 | 0.005097 | 0.004227 | -0.007651 | -0.012795 | 0.010638 | 0.003532 | 0.007714 | 0.008754 | -0.004805 | 0.006685 | 0.041987 | -0.005714 | -0.014964 | . 2019-07-02 0.000000 | 0.037889 | 0.000437 | 0.014335 | 0.021874 | -0.009699 | 0.008376 | 0.005071 | 0.008118 | 0.009317 | 0.011397 | -0.002105 | -0.002419 | 0.000546 | 0.013884 | 0.026437 | -0.003725 | -0.009535 | 0.035201 | -0.010690 | . 2019-07-03 0.021359 | 0.024772 | 0.026891 | 0.020916 | 0.023642 | 0.004828 | 0.021406 | 0.046922 | 0.017895 | 0.050822 | 0.026072 | 0.015295 | 0.007496 | -0.000546 | 0.017460 | 0.020829 | 0.028126 | -0.005901 | 0.018390 | 0.055735 | . 2019-07-04 -0.007605 | 0.005937 | 0.010219 | -0.019380 | 0.006086 | 0.005354 | 0.007194 | 0.031807 | 0.002051 | 0.022819 | 0.019810 | 0.002857 | -0.007222 | 0.004647 | -0.002692 | -0.006362 | 0.003004 | 0.008435 | -0.004429 | 0.049560 | . Calcoliamo poi la matrice di correlazione tra i rendimenti dei titoli e le componenti principali di quest&#39;ultima, utilizzando per l&#39;analisi le prime sei componenti, che spiegano circa l&#39;80% - 85% della varianza. . corr_mat = returns.corr() . . n_components = 6 pca = PCA(n_components=n_components) pc = pca.fit_transform(corr_mat) pc_df = pd.DataFrame( pc, returns.columns, [f&#39;PC{c}&#39; for c in range(n_components)] ) pc_df.index.name = &#39;Titolo&#39; pc_df.head() . . PC0 PC1 PC2 PC3 PC4 PC5 . Titolo . AMP.MI 0.690079 | -0.278930 | 0.001941 | -0.165962 | -0.210654 | -0.184466 | . ATL.MI 0.122440 | -0.401362 | 0.664491 | 0.236238 | 0.019327 | 0.070195 | . CNHI.MI -0.373885 | -0.201208 | 0.139796 | -0.158293 | 0.091624 | 0.253066 | . CPR.MI 0.634638 | 0.050509 | 0.136282 | -0.330863 | -0.010690 | -0.134976 | . ENEL.MI 0.247968 | 0.389905 | -0.023243 | 0.140282 | -0.121522 | 0.048196 | . var_df = pd.DataFrame( pca.explained_variance_ratio_, pc_df.columns, [&#39;Varianza cumulata&#39;] ) var_df.index.name = &#39;PC&#39; alt.Chart( var_df.cumsum().reset_index() ).mark_bar().encode( y=&#39;PC:N&#39;, x=&#39;Varianza cumulata:Q&#39;, tooltip=[&#39;PC&#39;, &#39;Varianza cumulata&#39;] ).properties( title=&#39;Varianza cumulata spiegata dalle componenti principali&#39; ).interactive() . . Possiamo analizzare quali sono le esposizioni dei singoli titoli alle componenti principali considerate - e in particolare alla prima, che spiega circa il 40% della varianza. Vediamo che le azioni si distribuiscono circa in due metà, con esposizioni negative e positive rispetto alla prima componente. . alt.Chart( pc_df.reset_index().melt( &#39;Titolo&#39;, var_name=&#39;PC&#39;, value_name=&#39;Esposizione&#39; ) ).mark_bar().encode( y=&#39;Titolo:N&#39;, x=&#39;Esposizione:Q&#39;, color=&#39;PC&#39;, row=&#39;PC&#39;, tooltip=[&#39;Titolo&#39;, &#39;PC&#39;, &#39;Esposizione&#39;] ).properties( title=&#39;Esposizione dei titoli alle componenti principali&#39; ).interactive() . . Facendo il grafico dei valori delle componenti principali, possiamo osservare che la prima componente è molto più volatile delle altre - per evitare che questo fenomeno influenzi la classificazione dei titoli, standardizziamo) i rendimenti delle componenti principali. . pc_px = returns.dot(pc_df).add(1).cumprod() pc_px.iloc[0, :] = 1 alt.Chart( pc_px.reset_index().melt( &#39;Date&#39;, var_name=&#39;PC&#39;, value_name=&#39;Prezzo&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Prezzo:Q&#39;, color=&#39;PC:N&#39;, tooltip=[&#39;Date&#39;, &#39;PC&#39;, &#39;Prezzo&#39;] ).properties( title=&#39;Andamento delle componenti principali&#39; ).interactive() . . def zscore(x): return (x - x.mean()) / x.std() zpc_df = pc_df.apply(zscore) . . Applichiamo a questo punto l&#39;algoritmo K-means per classificare i titoli in K gruppi simili. Dovendo decidere ex-ante il numero K di gruppi in cui partizionare lo spazio dei titoli, utilizziamo due metodi di ricerca dell&#39;ottimo applicandoli a valori di K compresi tra 2 e 10. Il primo metodo (inertia) fissa il punto di ottimo al &quot;gomito&quot; del grafico ottenuto - la linea più in alto nella figura sottostante. Il secondo metodo (silhouette) trova il numero di cluster ottimo in corrispondenza del massimo (locale) della funzione. In entrambi i casi, 7 sembra essere il valore ottimale per noi. . metrics_df = pd.DataFrame( index=range(2, 10), columns=[&#39;Inertia&#39;, &#39;Silhouette&#39;] ) metrics_df.index.name = &#39;N_clusters&#39; for i in metrics_df.index: km = KMeans(n_clusters=i) km.fit_predict(zpc_df) metrics_df.loc[i, &#39;Inertia&#39;] = km.inertia_ metrics_df.loc[i, &#39;Silhouette&#39;] = 100 * silhouette_score( zpc_df, km.labels_, metric=&#39;euclidean&#39; ) metrics_df . . Inertia Silhouette . N_clusters . 2 96.27442 | 14.174693 | . 3 79.545295 | 17.737655 | . 4 64.363608 | 23.572529 | . 5 48.211426 | 23.755179 | . 6 33.114174 | 32.99203 | . 7 22.553873 | 35.571988 | . 8 18.496865 | 33.401054 | . 9 15.065332 | 30.904044 | . alt.Chart( metrics_df.reset_index().melt( &#39;N_clusters&#39;, var_name=&#39;Metric&#39;, value_name=&#39;Score&#39; ) ).mark_line().encode( x=&#39;N_clusters:O&#39;, y=&#39;Score:Q&#39;, color=&#39;Metric:N&#39;, row=&#39;Metric:N&#39;, tooltip=[&#39;N_clusters&#39;, &#39;Score&#39;] ).properties( title=&#39;Metriche per la scelta del numero ottimale di clusters&#39; ).interactive() . . Raggruppiamo quindi i nostri venti titoli in 7 cluster utilizzando l&#39;algoritmo K-means. Essendo cluster multidimensionali, è complesso rappresentarli graficamente; tuttavia, è possibile costruire uno scatterplot che visualizzi i titoli raggruppati nei cluster sulla base delle prime due componenti principali. Possiamo notare nel grafico due cluster al margine sinistro che comprendono rispettivamente le grandi banche più Generali e Eni e i titoli collegati al gruppo Stellantis (ma anche Ferrari e STM, collocati in posizione più centrale). Altri due cluster facilmente identificabili sono nella parte alta del grafico (le aziende di pubblica utilità) e al margine destro (Campari e Amplifon, più Moncler, che si posiziona invece al centro del grafico). Infine gli ultimi tre cluster sono meno facilmente identificabili in base alle prime due componenti principali e comprendono rispettivamente Finecobank e Prysmian, la sola Atlantia e la sola Nexi. . n_clusters = 7 km_opt = KMeans(n_clusters=n_clusters) clusters = km_opt.fit_predict(zpc_df) cl_df = pc_df.copy() cl_df[&#39;Cluster&#39;] = clusters cl_df.head() . . PC PC0 PC1 PC2 PC3 PC4 PC5 Cluster . Titolo . AMP.MI 0.690079 | -0.278930 | 0.001941 | -0.165962 | -0.210654 | -0.184466 | 0 | . ATL.MI 0.122440 | -0.401362 | 0.664491 | 0.236238 | 0.019327 | 0.070195 | 2 | . CNHI.MI -0.373885 | -0.201208 | 0.139796 | -0.158293 | 0.091624 | 0.253066 | 5 | . CPR.MI 0.634638 | 0.050509 | 0.136282 | -0.330863 | -0.010690 | -0.134976 | 0 | . ENEL.MI 0.247968 | 0.389905 | -0.023243 | 0.140282 | -0.121522 | 0.048196 | 4 | . alt.Chart( cl_df.reset_index(), ).mark_circle().encode( x=&#39;PC0:Q&#39;, y=&#39;PC1:Q&#39;, color=&#39;Cluster:N&#39;, tooltip=[&#39;Titolo&#39;, &#39;Cluster&#39;] ).properties( title=&#39;Cluster rispetto alle prime due componenti principali&#39; ).interactive() . . Abbiamo così identificato gruppi di titoli omogenei in base a caratteristiche intrinseche del mercato, piuttosto che in base a elementi anagrafici o descrittivi come possono essere la nazione o il settore di appartenenza. Potremmo ad esempio costruire un portafoglio in cui ciascun cluster ottiene lo stesso peso e confrontarne l&#39;andamento con un portafoglio equipesato in base ai titoli. . wgt_df = pd.DataFrame(0.20 / cl_df[&#39;Cluster&#39;].value_counts()).rename(columns={&#39;Cluster&#39;: &#39;Peso&#39;}) . . pesi = pd.DataFrame(index=cl_df.index, columns=[&#39;Equal&#39;, &#39;Clustered&#39;]) pesi.Equal = 0.05 pesi.Clustered = cl_df.Cluster.map(wgt_df.Peso) . . test_df = pd.concat([ returns.dot(pesi.Equal).add(1).cumprod(), returns.dot(pesi.Clustered).add(1).cumprod() ], axis=1).rename(columns={0: &#39;Equal weights&#39;, 1: &#39;Clustered weights&#39;}) alt.Chart( test_df.reset_index().melt( &#39;Date&#39;, var_name=&#39;Portafoglio&#39;, value_name=&#39;Prezzo&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Prezzo:Q&#39;, color=&#39;Portafoglio:N&#39;, tooltip=[&#39;Date&#39;, &#39;Prezzo&#39;] ).properties( title=&#39;Backtest di due portafogli basati sui clusters&#39; ).interactive() . . Risulta che il portafoglio equipesato rispetto ai cluster ha ottenuto un risultato migliore negli ultimi due anni rispetto al portafoglio equipesato sui titoli, ma al prezzo di una maggiore volatilità. Naturalmente, non si tratta di una tecnica di ottimizzazione robusta, ma di un algoritmo che permette di individuare nuove dimensioni di analisi dei titoli e dei mercati che possono costituire la base per indagare più a fondo le motivazioni della similarità o dissimilarità di date azioni. .",
            "url": "https://marcobonifacio.github.io/blog/jupyter/scikit-learn/finanza/pca/machine%20learning/2021/06/28/componenti-principali-e-classificazione.html",
            "relUrl": "/jupyter/scikit-learn/finanza/pca/machine%20learning/2021/06/28/componenti-principali-e-classificazione.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Analizzare i mercati con le componenti principali",
            "content": "L&#39;analisi delle componenti principali (principal component analysis o PCA) è una tecnica statistica di riduzione della multidimensionalità dei dati ampiamente utilizzata anche nell&#39;analisi dei mercati finanziari. In questo post andremo ad applicarla a un semplice portafoglio di ETF anche in una versione &quot;dinamica&quot;, avvalendoci della tecnica delle medie mobili esponenziali. . import numpy as np import pandas as pd import altair as alt import yfinance as yf . . def normalize(series): return series.subtract(series.mean()).div(series.std()) def denormalize(series, mu, sd): return series.multiply(sd).add(mu) def format_pc(seq): return [f&#39;PC{n:02}&#39; for n, _ in enumerate(seq)] def df_dict(fun, seq): return {s: fun(s) for n, s in enumerate(seq)} def df_list(fun, seq): return [fun(s) for s in seq] def concat_dict(d): return pd.concat(d.values(), axis=0, keys=d.keys()) def dot(df1, df2): return df1.dot(df2.T) def dot3d(df1, df2): return df1.multiply(df2).sum(axis=1).unstack() def cov_matrix(df, norm, ewm, alpha=None): if ewm: if norm: return df.apply(normalize).ewm(alpha=alpha).cov() return df.ewm(alpha=alpha).cov() if norm: return df.apply(normalize).cov() return df.cov() def calc_variance(values): return (values / values.sum()).cumsum() def calc_pca(df): values, vectors = np.linalg.eig(df) return ( pd.Series(calc_variance(values), format_pc(df.index)), pd.DataFrame(vectors, format_pc(df.index), df.columns) ) def calc_ewm_pca(df): ans = df.fillna(0).groupby(level=0).apply(np.linalg.eig) def vectors(date): return pd.DataFrame( np.real(ans.loc[date][1]), format_pc(df.columns), df.columns ) def variance(date): return calc_variance(np.real(ans.loc[date][0])) dfs = df_dict(vectors, ans.index) return ( pd.DataFrame( df_list(variance, ans.index), ans.index, format_pc(df.columns) ), concat_dict(dfs) ) def calc_pc_returns(vectors, returns, norm): if not norm: return dot(returns, vectors) mu = dot(returns.mean(), vectors) sd = dot(returns.std(), vectors) return denormalize(dot(returns.apply(normalize), vectors), mu, sd) def calc_ewm_pc_returns(vectors, returns, norm): if not norm: return returns.T.apply(lambda c: vectors.xs(c.name).dot(c)).T mu = dot3d(vectors, returns.mean()) sd = dot3d(vectors, returns.std()) return denormalize(returns.apply(normalize).T.apply(lambda c: vectors.xs (c.name).dot(c)).T, mu, sd) . . Scarichiamo le serie storiche di alcuni ETF di Vanguard, tra cui i quattro che suggerisce come &quot;mattoncini di base&quot; nel portfolio builder dedicato agli investitori americani. Aggiungiamo due ETF dedicati al real estate e alle commodities e scarichiamo anche la serie dei prezzi dell&#39;indice VIX, la volatilità del listino statunitense. . tickers = [ &#39;VTI&#39;, # Vanguard Total Stock Market &#39;VXUS&#39;, # Vanguard Total International Stock &#39;BND&#39;, # Vanguard Total Bond Market &#39;BNDX&#39;, # Vanguard Total International Bond &#39;VNQ&#39;, # Vanguard Real Estate Index Fund &#39;GSG&#39;, # iShares S&amp;P GSCI Commodity-Indexed Trust &#39;^VIX&#39; # Indice VIX ] data = yf.download(tickers=tickers, period=&#39;2y&#39;) prices = data.xs(&#39;Adj Close&#39;, axis=1, level=0) . . [*********************100%***********************] 7 of 7 completed . Ribasiamo i prezzi dei nostri ETF a 100 e disegniamo un primo grafico per vederne l&#39;andamento storico. . rebased_prices = prices.iloc[:, :6].div(prices.iloc[0, :6]).mul(100) alt.Chart( rebased_prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;ETF&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;ETF:N&#39;, row=&#39;ETF:N&#39;, tooltip=[&#39;ETF&#39;, &#39;Date&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento prezzi ETF ribasati negli ultimi due anni&#39; ).interactive() . . Calcoliamo i rendimenti giornalieri delle serie storiche e la matrice di correlazione tra gli ETF. . returns = rebased_prices.pct_change() returns.corr() . . BND BNDX GSG VNQ VTI VXUS . BND 1.000000 | 0.648077 | 0.061670 | 0.211954 | 0.188501 | 0.214418 | . BNDX 0.648077 | 1.000000 | 0.037991 | 0.285365 | 0.206667 | 0.236108 | . GSG 0.061670 | 0.037991 | 1.000000 | 0.391428 | 0.492088 | 0.500902 | . VNQ 0.211954 | 0.285365 | 0.391428 | 1.000000 | 0.860074 | 0.808887 | . VTI 0.188501 | 0.206667 | 0.492088 | 0.860074 | 1.000000 | 0.923701 | . VXUS 0.214418 | 0.236108 | 0.500902 | 0.808887 | 0.923701 | 1.000000 | . Per calcolare le componenti principali, è preferibile utilizzare una matrice di covarianza, a partire dai rendimenti standardizzati), ossia da serie con media nulla e varianza pari a 1. . covmat = cov_matrix(returns, True, False) . . Il calcolo matriciale delle componenti principali prevede di trovare autovalori e autovettori della matrice di covarianza, qui calcoliamo già un vettore di autovalori cumulati, che andiamo poi a disegnare come grafico a barre. . variance, vectors = calc_pca(covmat) . . Dal grafico possiamo vedere che la prima componente principale (PC00), spiega poco meno del 60% della varianza complessiva dei rendimenti degli indici, mentre la somma delle prime due componenti portano la varianza spiegata quasi all&#39;80%. Chiaramente, in questo portafoglio di 6 strumenti, la necessità di ridurre la dimensionalità è modesta, ma la tecnica è utile laddove ci siano decine o centinaia di indici o titoli per identificare le componenti principali da utilizzare poi per ulteriori analisi. . alt.Chart( variance.rename(&#39;Percentuale&#39;).to_frame().reset_index() ).mark_bar().encode( y=&#39;index:N&#39;, x=&#39;Percentuale:Q&#39;, tooltip=[&#39;index&#39;, &#39;Percentuale&#39;] ).properties( title=&#39;Varianza cumulata spiegata dalle componenti principali&#39; ).interactive() . . A partire dalla matrice degli autovettori possiamo inoltre ricostruire i &quot;rendimenti&quot; delle componenti principali e i valori delle serie, come se fossero prezzi di serie finanziarie. Lo facciamo nel grafico successivo, ricordando che le prime due componenti principali sono contrassegnate dalla dicitura PC00 e PC01. . pc_returns = calc_pc_returns(vectors, returns, True) pc_prices = pc_returns.add(1).cumprod().multiply(100) alt.Chart( pc_prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;PC&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;PC:N&#39;, row=&#39;PC:N&#39;, tooltip=[&#39;PC&#39;, &#39;Date&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento valori componenti principali negli ultimi due anni&#39; ).interactive() . . Le componenti principali sono calcolate sul periodo di analisi e dipendono pertanto dalla finestra temporale scelta. Ci si potrebbe domandare: è possibile averne una versione dinamica, rolling che non dipenda da una finestra temporale fissa? Proviamo ad applicare la tecnica calcolando delle matrici di covarianza rolling con la metodologia delle medie mobili esponenziali, applicando un fattore di decadimento (alpha), pari a 0.06 - per ogni osservazione, il 94% deriva dal dato precedente e il 6% è introdotto sulla base del nuovo dato. Disegniamo poi il grafico della varianza spiegata dalla prima componente principale, mettendolo a confronto con l&#39;andamento dell&#39;indice VIX. . ewm_covmat = cov_matrix(returns, True, True, 0.06) . . Interessante: la varianza spiegata dalla prima componente principale arriva fino a più dell&#39;80% (mentre l&#39;ultimo dato è al 40%) esattamente in coincidenza con il picco del VIX, ossia della correzione dei mercati di marzo 2020. Il che è in totale accordo con l&#39;ipotesi che la correlazione dei mercati e delle classi di attivi cresca nei momenti di maggiore volatilità e di correzione. . import warnings warnings.filterwarnings(&#39;ignore&#39;) ewm_variance, ewm_vectors = calc_ewm_pca(ewm_covmat) alt.Chart( pd.concat([100 * ewm_variance.iloc[20:, 0], prices.iloc[:, 6]], axis=1).reset_index().melt( &#39;Date&#39;, var_name=&#39;Index&#39;, value_name=&#39;Value&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Value:Q&#39;, color=&#39;Index:N&#39;, row=&#39;Index:N&#39;, tooltip=[&#39;Index&#39;, &#39;Date&#39;, &#39;Value&#39;] ).properties( title=&#39;Confronto andamento prima PC e indice VIX&#39; ).interactive() . . Possiamo infine calcolare gli andamenti storici delle componenti principali, che saranno certamente differenti da quelle &quot;statiche&quot;; in particolare, appaiono più volatili in coincidenza con il periodo di crisi di febbraio - aprile 2020. . ewm_pc_returns = calc_ewm_pc_returns(ewm_vectors, returns, False) ewm_pc_prices = ewm_pc_returns.add(1).cumprod().multiply(100) alt.Chart( ewm_pc_prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;PC&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;PC:N&#39;, row=&#39;PC:N&#39;, tooltip=[&#39;PC&#39;, &#39;Date&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento valori componenti principali EW negli ultimi due anni&#39; ).interactive() . .",
            "url": "https://marcobonifacio.github.io/blog/jupyter/pandas/finanza/pca/statistica/2021/06/24/analizzare-i-mercati-con-le-componenti-principali.html",
            "relUrl": "/jupyter/pandas/finanza/pca/statistica/2021/06/24/analizzare-i-mercati-con-le-componenti-principali.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Prezzi e rendimenti con Pandas",
            "content": "fastpages, la piattaforma sulla quale è costruito questo blog, permette di integrare direttamente dei notebook di Jupyter nei post. Per provare questa funzionalità, costruiamo un breve notebook su come gestire prezzi e rendimenti delle azioni con Pandas, in alternativa a Excel. In questo modo, potremo osservare in azione alcuni degli strumenti che ho descritto in un mio post precedente. . import pandas as pd import altair as alt import yfinance as yf . . Scarichiamo da Yahoo! Finance i prezzi di quattro azioni statunitensi, identificate attraverso i loro ticker: Apple (AAPL), Microsoft (MSFT), McDonald&#39;s (MCD) e Coca-Cola (KO). Due azioni della new economy e due della old, come si diceva qualche anno fa. . tickers = &#39;AAPL MSFT MCD KO&#39; data = yf.download(tickers=tickers, period=&#39;2y&#39;) . . [*********************100%***********************] 4 of 4 completed . A questo punto, eliminiamo dal database che abbiamo costruito i prezzi open, high, low e il volume negoziato per tenerci solo i prezzi di chiusura, rettificati dei dividendi eventualmente distribuiti e delle corporate action. Vediamo le prime righe del database dei prezzi. . prices = data.xs(&#39;Adj Close&#39;, axis=1, level=0) prices.tail() . . AAPL KO MCD MSFT . Date . 2021-06-18 130.460007 | 53.770000 | 229.619995 | 259.429993 | . 2021-06-21 132.300003 | 54.360001 | 232.899994 | 262.630005 | . 2021-06-22 133.979996 | 54.560001 | 233.880005 | 265.510010 | . 2021-06-23 133.699997 | 54.119999 | 233.240005 | 265.269989 | . 2021-06-24 134.380005 | 54.115002 | 234.320007 | 267.234985 | . Possiamo fare un primo grafico dell&#39;andamento dei prezzi delle quattro azioni negli ultimi due anni, utilizzando Altair, una libreria che permette di produrre grafici interattivi. . alt.Chart( prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento prezzi negli ultimi due anni&#39; ).interactive() . . Per avere la possibilità di confrontare i quattro grafici, ribasiamo i dati facendo partire gli andamenti da quota 100. . rebased_prices = prices.div(prices.iloc[0, :]).mul(100) alt.Chart( rebased_prices.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Price&#39; ) ).mark_line().encode( x=&#39;Date:T&#39;, y=&#39;Price:Q&#39;, color=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Price&#39;] ).properties( title=&#39;Andamento prezzi ribasati negli ultimi due anni&#39; ).interactive() . . Passiamo ora ai rendimenti e calcoliamo i rendimenti mensili delle quattro azioni. . monthly_returns = prices.resample(&#39;M&#39;).last().pct_change() . . Possiamo fare un grafico a barre dei rendimenti appena calcolati, divisi per mese. . alt.Chart( monthly_returns.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Return&#39; ) ).mark_bar().encode( y=&#39;Stock:N&#39;, x=&#39;Return:Q&#39;, color=&#39;Stock:N&#39;, row=&#39;Date:T&#39;, tooltip=[&#39;Stock&#39;, &#39;Return&#39;] ).properties( title=&#39;Rendimenti percentuali per mese&#39; ).interactive() . . Oppure lo stesso grafico raggruppato per titolo, dove notiamo come Apple sia stato il titolo più volatile negli ultimi due anni, ma Coca-Cola e McDonald&#39;s abbiano avuto i maggiori drawdown. . alt.Chart( monthly_returns.reset_index().melt( &#39;Date&#39;, var_name=&#39;Stock&#39;, value_name=&#39;Return&#39; ) ).mark_bar().encode( x=&#39;Date:T&#39;, y=&#39;Return:Q&#39;, color=&#39;Stock:N&#39;, row=&#39;Stock:N&#39;, tooltip=[&#39;Stock&#39;, &#39;Return&#39;] ).properties( title=&#39;Rendimenti percentuali per mese&#39; ).interactive() . . Ci fermiamo qui con questo post di prova, che mostra le potenzialità di analisi esplorativa di Pandas, in grado in poche righe di codice di scaricare, elaborare e visualizzare serie storiche di dati con grande facilità. .",
            "url": "https://marcobonifacio.github.io/blog/jupyter/pandas/finanza/2021/06/23/prezzi-e-rendimenti-con-pandas.html",
            "relUrl": "/jupyter/pandas/finanza/2021/06/23/prezzi-e-rendimenti-con-pandas.html",
            "date": " • Jun 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Python per la finanza",
            "content": "Python è per me diventato negli anni uno strumento indispensabile di lavoro, soprattutto quando i dati da gestire superano le capacità di Excel o per effettuare alcune analisi statistiche un po’ più sofisticate del solito. Tuttavia, mi sono accorto che costruire un ambiente di lavoro efficace ed efficiente basato su Python non è sempre intuitivo, per cui proverò in questo post a condividere alcune delle cose che ho imparato nel tempo per giungere a una configurazione soddisfacente di questo importante strumento, soprattutto per chi lavora in ambito finanziario o ha comunque necessità di analizzare dei dati. . La distribuzione di Python più interessante per chi lavora con i dati (o fa data science, come si dice adesso) è sicuramente Anaconda, che però installa di default fin troppe librerie scientifiche per i miei gusti. Esiste una versione minimal di Anaconda, miniconda, che installa solo l’interprete del linguaggio e un set minimo di librerie per il funzionamento del package manager, lasciando all’utente la possibilità di installare poi solo gli ulteriori pacchetti effettivamente necessari per ciascun progetto. Qui però iniziano le difficoltà. Se da un lato conda, rispetto a pip, il package manager ufficiale di Python, permette una gestione delle librerie più agevole, dall’altro il numnero di pacchetti disponibili è ben inferiore. Per fortuna, un’ampia community si è fatta carico di ampliare la disponibilità di librerie sotto conda attraverso il canale conda-forge. Mischiare però le distribuzioni “ufficiali” di Anaconda con quelle di conda-forge è sconsigliabile e nel tempo rischia di corrompere i vari ambienti di lavoro di Python. Per questo ho recentemente scoperto con soddifazione miniforge, un clone di miniconda che scarica direttamente i pacchetti di Python dal canale della community di conda-forge1. Alla fine, quindi, sto usando con soddisfazione da alcuni mesi questa distribuzione, con impostazioni identiche tra i vari dispositivi che utilizzo in modo da avere ambienti di lavoro intercambiabili. . Passando alle librerie vere e proprie, la prima cosa da installare, secondo me, è la triade numpy - pandas - matplotlib. Altamente integrati tra loro, questi tre pacchetti soddisfano tutte le esigenze di analisi e visualizzazione dei dati, sostituendo più che degnamente gli enormi fogli Excel con centinaia di collegamenti che vanno velocemente fuori controllo. A questa configurazione base si possono poi aggiungere numerose altre librerie più specializzate, tra cui, ad esempio, scipy per le procedure di ottimizzazione o scikit-learn per classificazione e regressione (o - più pomposamente - per il machine learning). . Tutto questo può essere gestito tramite un normale editor di testo o l’interprete interattivo di Python, ma è probabilmente più comodo fare le prime analisi di un progetto attraverso un notebook, un’applicazione web che consente di mescolare codice, risultati, visualizzazioni e testi in un unico documento. Qui entra in gioco jupyter, progetto open source di notebook multi-linguaggio, che nell’ultima versione - JupyterLab - integra anche un terminale e vari plugin che lo rendono un vero e proprio ambiente web interattivo di sviluppo. Sebbene i notebook, estremamente versatili proprio per la capacità di integrare codice, testo e grafici in un unico documento, possano essere strumenti decisamente sofisticati in grado di generare un prodotto finito molto elegante, si prestano anche bene per le prime analisi esplorative e per costruire veloci prototipi di progetto, grazie all’immediatezza con cui in poche righe di codice si può caricare una tabella di dati, elaborarla e produrre qualche grafico a corredo. . Il problema dei notebook è che si prestano a uno stile di programmazione imperativo che rischia di produrre documenti “pasticciati” in caso di progetti più complessi e strutturati. Per questi ultimi, sono però disponibili strumenti che permettono di creare con relativa facilità delle vere e proprie applicazioni web in cui risulta più semplice separare parametri e algoritmi e strutturare in modo più funzionale il codice. Tra questi strumenti, citerei in particolare Streamlit che ho recentemente utilizzato per costruire una dashboard nella quale riunire tutti i task quotidiani in modo da avere un vero e proprio pannello di controllo delle attività ricorrenti nella mia giornata lavorativa. Quello che mi ha sorpreso di Streamlit è l’assoluta facilità con cui è stato possibile integrare elementi interattivi nel codice per costruire pagine web dinamiche; inoltre - ma questa è una caratteristica che condivide con molti altri pacchetti - come sia stato possibile costruire delle vere e proprie web app solo con Python2. . Ecco, in sintesi questo è il mio ambiente di lavoro dietro lo schermo. Peraltro, io sono un tipo curioso e ho provato diversi strumenti in questi anni, prima di arrivare a una configurazione più o meno stabile che ho cercato qui di descrivere. E non è detto che non troverò qualcosa di interessante anche domani, per cui cambierò qualcosa e avrò una scusa per tornare a parlarne. . Anche se in realtà lo stesso risultato si può ottenere in miniconda settando il canale conda-forge per primo. &#8617; . | Ovviamente, dietro c’è anche tanto Javascript, ma se ne occupa direttamente Streamlit o chi per lui. &#8617; . |",
            "url": "https://marcobonifacio.github.io/blog/python/finanza/2021/05/05/python-per-la-finanza.html",
            "relUrl": "/python/finanza/2021/05/05/python-per-la-finanza.html",
            "date": " • May 5, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Laureato in Economia Aziendale alla Bocconi, lavoro da circa 25 anni nel settore finanziario, prima come analista, poi come risk manager. . Appassionato di programmazione, uso Python per mestiere e per diletto. In questo blog, provo a raccontare le mie scoperte su programmazione, finanza, ma non solo. . Restiamo in contatto! .",
          "url": "https://marcobonifacio.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marcobonifacio.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}